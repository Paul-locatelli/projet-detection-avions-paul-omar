{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paul-locatelli/projet-detection-avions-paul-omar/blob/main/Model_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os, glob, pathlib, random, shutil\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/Final_product\"\n",
        "DATASET_ROOT = os.path.join(ROOT, \"DataSet\")\n",
        "RAW_DIR = os.path.join(DATASET_ROOT, \"raw\")\n",
        "\n",
        "OUT_CROPS = os.path.join(DATASET_ROOT, \"cls_crops\")\n",
        "OUT_SPLIT1N = os.path.join(DATASET_ROOT, \"split1n\")\n",
        "OUT_KEYS = os.path.join(DATASET_ROOT, \"splits_keys\")\n",
        "\n",
        "for d in [OUT_CROPS, OUT_SPLIT1N, OUT_KEYS]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def norm_stem(p):\n",
        "    s = pathlib.Path(p).stem.lower()\n",
        "    s = s.replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\")\n",
        "    return s\n",
        "\n",
        "img_files = sorted(\n",
        "    glob.glob(os.path.join(RAW_DIR, \"**\", \"*.jpg\"), recursive=True) +\n",
        "    glob.glob(os.path.join(RAW_DIR, \"**\", \"*.jpeg\"), recursive=True) +\n",
        "    glob.glob(os.path.join(RAW_DIR, \"**\", \"*.png\"), recursive=True)\n",
        ")\n",
        "xml_files = sorted(glob.glob(os.path.join(RAW_DIR, \"**\", \"*.xml\"), recursive=True))\n",
        "\n",
        "assert len(img_files) > 0, \"No raw images found under DataSet/raw\"\n",
        "assert len(xml_files) > 0, \"No raw xml found under DataSet/raw\"\n",
        "\n",
        "img_map = {}\n",
        "for p in img_files:\n",
        "    k = norm_stem(p)\n",
        "    if k not in img_map:\n",
        "        img_map[k] = p\n",
        "\n",
        "xml_map = {}\n",
        "for p in xml_files:\n",
        "    k = norm_stem(p)\n",
        "    if k not in xml_map:\n",
        "        xml_map[k] = p\n",
        "\n",
        "matched = sorted(set(img_map.keys()) & set(xml_map.keys()))\n",
        "assert len(matched) > 0, \"No matched image/xml pairs. Filenames don't align.\"\n",
        "\n",
        "print(\"Raw images:\", len(img_map))\n",
        "print(\"Raw xml:\", len(xml_map))\n",
        "print(\"Matched pairs:\", len(matched))\n",
        "\n",
        "def parse_voc(xml_path):\n",
        "    root = ET.parse(xml_path).getroot()\n",
        "    objs = []\n",
        "    for obj in root.findall(\"object\"):\n",
        "        name = obj.findtext(\"name\")\n",
        "        bnd = obj.find(\"bndbox\")\n",
        "        if bnd is None:\n",
        "            continue\n",
        "        x1 = int(float(bnd.findtext(\"xmin\")))\n",
        "        y1 = int(float(bnd.findtext(\"ymin\")))\n",
        "        x2 = int(float(bnd.findtext(\"xmax\")))\n",
        "        y2 = int(float(bnd.findtext(\"ymax\")))\n",
        "        objs.append((name, x1, y1, x2, y2))\n",
        "    return objs\n",
        "\n",
        "label_set = set()\n",
        "empty_ann = 0\n",
        "for k in matched:\n",
        "    objs = parse_voc(xml_map[k])\n",
        "    if len(objs) == 0:\n",
        "        empty_ann += 1\n",
        "    for name, *_ in objs:\n",
        "        if name is not None:\n",
        "            label_set.add(name)\n",
        "\n",
        "classes = sorted(label_set)\n",
        "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
        "\n",
        "print(\"Detected classes:\", len(classes))\n",
        "print(\"Empty-annotation xml:\", empty_ann)\n",
        "print(\"Classes:\", classes)\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(matched)\n",
        "\n",
        "n = len(matched)\n",
        "n_train = int(0.8 * n)\n",
        "n_val = int(0.1 * n)\n",
        "\n",
        "train_keys = matched[:n_train]\n",
        "val_keys = matched[n_train:n_train+n_val]\n",
        "test_keys = matched[n_train+n_val:]\n",
        "\n",
        "print(\"Split sizes:\", len(train_keys), len(val_keys), len(test_keys))\n",
        "\n",
        "def write_keys(keys, path):\n",
        "    with open(path, \"w\") as f:\n",
        "        for k in keys:\n",
        "            f.write(k + \"\\n\")\n",
        "\n",
        "os.makedirs(OUT_KEYS, exist_ok=True)\n",
        "write_keys(train_keys, os.path.join(OUT_KEYS, \"train.txt\"))\n",
        "write_keys(val_keys, os.path.join(OUT_KEYS, \"val.txt\"))\n",
        "write_keys(test_keys, os.path.join(OUT_KEYS, \"test.txt\"))\n",
        "\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    for c in classes:\n",
        "        os.makedirs(os.path.join(OUT_CROPS, split, c), exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUT_SPLIT1N, split, \"images\"), exist_ok=True)\n",
        "\n",
        "def safe_crop(img, x1, y1, x2, y2):\n",
        "    w, h = img.size\n",
        "    x1 = max(0, min(int(x1), w-1))\n",
        "    y1 = max(0, min(int(y1), h-1))\n",
        "    x2 = max(0, min(int(x2), w))\n",
        "    y2 = max(0, min(int(y2), h))\n",
        "    if x2 <= x1 or y2 <= y1:\n",
        "        x2 = min(w, x1+1)\n",
        "        y2 = min(h, y1+1)\n",
        "    return img.crop((x1, y1, x2, y2))\n",
        "\n",
        "def process_split(keys, split_name, copy_images=True):\n",
        "    saved_crops = 0\n",
        "    saved_imgs = 0\n",
        "    for k in keys:\n",
        "        img_path = img_map[k]\n",
        "        xml_path = xml_map[k]\n",
        "\n",
        "        if copy_images:\n",
        "            dst_img = os.path.join(OUT_SPLIT1N, split_name, \"images\", os.path.basename(img_path))\n",
        "            if not os.path.exists(dst_img):\n",
        "                shutil.copy2(img_path, dst_img)\n",
        "                saved_imgs += 1\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        objs = parse_voc(xml_path)\n",
        "        for j, (name, x1, y1, x2, y2) in enumerate(objs):\n",
        "            if name not in class_to_idx:\n",
        "                continue\n",
        "            crop = safe_crop(img, x1, y1, x2, y2)\n",
        "            out_name = f\"{pathlib.Path(img_path).stem}__{j}.jpg\"\n",
        "            out_path = os.path.join(OUT_CROPS, split_name, name, out_name)\n",
        "            crop.save(out_path, quality=95)\n",
        "            saved_crops += 1\n",
        "    return saved_crops, saved_imgs\n",
        "\n",
        "for split_name, keys in [(\"train\", train_keys), (\"val\", val_keys), (\"test\", test_keys)]:\n",
        "    crops_n, imgs_n = process_split(keys, split_name, copy_images=True)\n",
        "    print(split_name, \"| crops:\", crops_n, \"| images copied:\", imgs_n)\n",
        "\n",
        "meta_path = os.path.join(DATASET_ROOT, \"dataset_meta.txt\")\n",
        "with open(meta_path, \"w\") as f:\n",
        "    f.write(\"classes=\" + \",\".join(classes) + \"\\n\")\n",
        "    f.write(\"num_classes=\" + str(len(classes)) + \"\\n\")\n",
        "    f.write(\"matched_pairs=\" + str(len(matched)) + \"\\n\")\n",
        "\n",
        "print(\"Crops folder:\", OUT_CROPS)\n",
        "print(\"Split images folder:\", OUT_SPLIT1N)\n",
        "print(\"Keys folder:\", OUT_KEYS)\n",
        "print(\"Meta:\", meta_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrE800H8xb2Y",
        "outputId": "f846cc11-052f-4ced-97cf-1024c94c41bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Raw images: 1331\n",
            "Raw xml: 1331\n",
            "Matched pairs: 1331\n",
            "Detected classes: 20\n",
            "Empty-annotation xml: 0\n",
            "Classes: ['A1', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A2', 'A20', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9']\n",
            "Split sizes: 1064 133 134\n",
            "train | crops: 6220 | images copied: 1064\n",
            "val | crops: 876 | images copied: 133\n",
            "test | crops: 774 | images copied: 134\n",
            "Crops folder: /content/drive/MyDrive/Final_product/DataSet/cls_crops\n",
            "Split images folder: /content/drive/MyDrive/Final_product/DataSet/split1n\n",
            "Keys folder: /content/drive/MyDrive/Final_product/DataSet/splits_keys\n",
            "Meta: /content/drive/MyDrive/Final_product/DataSet/dataset_meta.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, pathlib\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import torchvision.transforms as T\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/Final_product\"\n",
        "DATASET_ROOT = os.path.join(ROOT, \"DataSet\")\n",
        "RAW_DIR = os.path.join(DATASET_ROOT, \"raw\")\n",
        "KEYS_DIR = os.path.join(DATASET_ROOT, \"splits_keys\")\n",
        "META_PATH = os.path.join(DATASET_ROOT, \"dataset_meta.txt\")\n",
        "\n",
        "MODELS_DIR = os.path.join(ROOT, \"models\")\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "BEST_PATH = os.path.join(MODELS_DIR, \"best_faster_rcnn_raw.pth\")\n",
        "\n",
        "TRAIN_TXT = os.path.join(KEYS_DIR, \"train.txt\")\n",
        "VAL_TXT   = os.path.join(KEYS_DIR, \"val.txt\")\n",
        "TEST_TXT  = os.path.join(KEYS_DIR, \"test.txt\")\n",
        "\n",
        "assert os.path.isdir(RAW_DIR), f\"Missing {RAW_DIR}\"\n",
        "assert os.path.exists(TRAIN_TXT) and os.path.exists(VAL_TXT) and os.path.exists(TEST_TXT), \"Missing splits_keys/*.txt\"\n",
        "assert os.path.exists(META_PATH), f\"Missing {META_PATH}\"\n",
        "\n",
        "print(\"RAW_DIR:\", RAW_DIR)\n",
        "print(\"KEYS_DIR:\", KEYS_DIR)\n",
        "print(\"META:\", META_PATH)\n",
        "print(\"SAVE:\", BEST_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ev3fD_M1Zqv",
        "outputId": "2d6c13d1-9e11-477e-d021-2eb1c916b266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW_DIR: /content/drive/MyDrive/Final_product/DataSet/raw\n",
            "KEYS_DIR: /content/drive/MyDrive/Final_product/DataSet/splits_keys\n",
            "META: /content/drive/MyDrive/Final_product/DataSet/dataset_meta.txt\n",
            "SAVE: /content/drive/MyDrive/Final_product/models/best_faster_rcnn_raw.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(META_PATH, \"r\") as f:\n",
        "    lines = f.read().splitlines()\n",
        "\n",
        "classes_line = [l for l in lines if l.startswith(\"classes=\")][0]\n",
        "classes = classes_line.split(\"=\", 1)[1].split(\",\")\n",
        "classes = [c for c in classes if c]\n",
        "\n",
        "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
        "\n",
        "print(\"Num classes:\", len(classes))\n",
        "print(\"Classes:\", classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR-iIwa41hDO",
        "outputId": "559432e0-4120-42b8-82ed-cad8f7ea39d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num classes: 20\n",
            "Classes: ['A1', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A2', 'A20', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def norm_stem_from_path(p):\n",
        "    s = pathlib.Path(p).stem.lower()\n",
        "    s = s.replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\")\n",
        "    return s\n",
        "\n",
        "img_files = sorted(\n",
        "    glob.glob(os.path.join(RAW_DIR, \"**\", \"*.jpg\"), recursive=True) +\n",
        "    glob.glob(os.path.join(RAW_DIR, \"**\", \"*.jpeg\"), recursive=True) +\n",
        "    glob.glob(os.path.join(RAW_DIR, \"**\", \"*.png\"), recursive=True)\n",
        ")\n",
        "xml_files = sorted(glob.glob(os.path.join(RAW_DIR, \"**\", \"*.xml\"), recursive=True))\n",
        "\n",
        "assert len(img_files) > 0, \"No images in raw/\"\n",
        "assert len(xml_files) > 0, \"No xml in raw/\"\n",
        "\n",
        "img_map = {}\n",
        "for p in img_files:\n",
        "    k = norm_stem_from_path(p)\n",
        "    if k not in img_map:\n",
        "        img_map[k] = p\n",
        "\n",
        "xml_map = {}\n",
        "for p in xml_files:\n",
        "    k = norm_stem_from_path(p)\n",
        "    if k not in xml_map:\n",
        "        xml_map[k] = p\n",
        "\n",
        "print(\"Raw images:\", len(img_map))\n",
        "print(\"Raw xml:\", len(xml_map))\n",
        "\n",
        "# juste pour v√©rifier que √ßa colle √† tes 1331\n",
        "inter = len(set(img_map.keys()) & set(xml_map.keys()))\n",
        "print(\"Intersection img/xml:\", inter)\n",
        "assert inter > 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns759VI41kPa",
        "outputId": "029eee90-a763-452a-ac95-9fcff1dda9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw images: 1331\n",
            "Raw xml: 1331\n",
            "Intersection img/xml: 1331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_keys(p):\n",
        "    with open(p, \"r\") as f:\n",
        "        return [line.strip() for line in f if line.strip()]\n",
        "\n",
        "train_keys = read_keys(TRAIN_TXT)\n",
        "val_keys   = read_keys(VAL_TXT)\n",
        "test_keys  = read_keys(TEST_TXT)\n",
        "\n",
        "print(\"Split sizes:\", len(train_keys), len(val_keys), len(test_keys))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "064jNy8w1mcQ",
        "outputId": "445b90bb-688a-4705-bedb-2eaa5e9053d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split sizes: 1064 133 134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_voc(xml_path):\n",
        "    root = ET.parse(xml_path).getroot()\n",
        "    objs = []\n",
        "    for obj in root.findall(\"object\"):\n",
        "        name = obj.findtext(\"name\")\n",
        "        bnd = obj.find(\"bndbox\")\n",
        "        if bnd is None:\n",
        "            continue\n",
        "        x1 = int(float(bnd.findtext(\"xmin\")))\n",
        "        y1 = int(float(bnd.findtext(\"ymin\")))\n",
        "        x2 = int(float(bnd.findtext(\"xmax\")))\n",
        "        y2 = int(float(bnd.findtext(\"ymax\")))\n",
        "        objs.append((name, x1, y1, x2, y2))\n",
        "    return objs\n"
      ],
      "metadata": {
        "id": "1fRCi6pv1ooc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VOCDatasetRAW(Dataset):\n",
        "    def __init__(self, keys, img_map, xml_map, class_to_idx):\n",
        "        self.keys = keys\n",
        "        self.img_map = img_map\n",
        "        self.xml_map = xml_map\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.tf = T.ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        k = self.keys[idx]\n",
        "\n",
        "        img_path = self.img_map[k]\n",
        "        xml_path = self.xml_map[k]\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        boxes, labels = [], []\n",
        "        for name, x1, y1, x2, y2 in parse_voc(xml_path):\n",
        "            if name not in self.class_to_idx:\n",
        "                continue\n",
        "            if x2 <= x1 or y2 <= y1:\n",
        "                continue\n",
        "            boxes.append([x1, y1, x2, y2])\n",
        "            labels.append(self.class_to_idx[name] + 1)\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        if boxes.numel() == 0:\n",
        "            boxes = boxes.reshape(0, 4)\n",
        "            labels = labels.reshape(0,)\n",
        "\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if boxes.shape[0] else torch.zeros((0,), dtype=torch.float32)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": torch.tensor([idx]),\n",
        "            \"iscrowd\": torch.zeros((labels.shape[0],), dtype=torch.int64),\n",
        "            \"area\": area\n",
        "        }\n",
        "\n",
        "        return self.tf(img), target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs, targets = zip(*batch)\n",
        "    return list(imgs), list(targets)\n"
      ],
      "metadata": {
        "id": "0AlVMcVq1q6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = VOCDatasetRAW(train_keys, img_map, xml_map, class_to_idx)\n",
        "val_ds   = VOCDatasetRAW(val_keys,   img_map, xml_map, class_to_idx)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=2, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader))\n",
        "print(\"Val batches  :\", len(val_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQeiFIwJ1xuv",
        "outputId": "b132e743-8751-41ea-8c79-074c401669dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 532\n",
            "Val batches  : 67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "num_classes = len(classes) + 1  # + background\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "in_feat = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)\n",
        "model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "print(\"RCNN ready with\", num_classes, \"classes (incl background)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOBEzI3m1z7t",
        "outputId": "2e5fe337-b804-405c-f055-47e0850517da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 160M/160M [00:00<00:00, 209MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RCNN ready with 21 classes (incl background)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(loader):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for imgs, targets in loader:\n",
        "        imgs = [i.to(device) for i in imgs]\n",
        "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(imgs, targets)\n",
        "        loss = sum(loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += loss.item()\n",
        "    return total / max(1, len(loader))\n",
        "\n",
        "@torch.no_grad()\n",
        "def val_epoch(loader):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for imgs, targets in loader:\n",
        "        imgs = [i.to(device) for i in imgs]\n",
        "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(imgs, targets)\n",
        "        loss = sum(loss_dict.values())\n",
        "        total += loss.item()\n",
        "    return total / max(1, len(loader))\n",
        "\n",
        "EPOCHS = 10\n",
        "best_val = 1e9\n",
        "\n",
        "print(\"\\nüöÄ DETECTOR TRAINING START\\n\")\n",
        "\n",
        "for e in range(1, EPOCHS+1):\n",
        "    tr = train_epoch(train_loader)\n",
        "    va = val_epoch(val_loader)\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    print(f\"epoch {e}/{EPOCHS} | train_loss={tr:.4f} | val_loss={va:.4f}\")\n",
        "\n",
        "    if va < best_val:\n",
        "        best_val = va\n",
        "        torch.save({\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"classes\": classes,\n",
        "            \"class_to_idx\": class_to_idx\n",
        "        }, BEST_PATH)\n",
        "        print(\"‚úÖ saved best ->\", BEST_PATH)\n",
        "\n",
        "print(\"\\nüèÅ DONE\")\n",
        "print(\"Best val loss:\", best_val)\n",
        "print(\"Detector saved:\", BEST_PATH)\n"
      ],
      "metadata": {
        "id": "FxvJzUju1397"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objectif  \n",
        "L‚Äôobjectif de cette partie est de mettre en place un mod√®le de d√©tection d‚Äôa√©ronefs capable de localiser et identifier les avions pr√©sents dans une image. Cette √©tape constitue la premi√®re brique du pipeline global, avant la phase de classification fine bas√©e sur des crops.\n",
        "\n",
        "Donn√©es utilis√©es  \n",
        "Le d√©tecteur est entra√Æn√© √† partir des donn√©es brutes non modifi√©es (RAW). Les images originales et leurs annotations au format Pascal VOC (XML) sont stock√©es dans le dossier Final_product/DataSet/raw/.  \n",
        "Le jeu de donn√©es contient 1331 images annot√©es, chacune associ√©e √† un fichier XML d√©crivant les classes et les coordonn√©es des bounding boxes. Le nombre total de classes d√©tect√©es est de 20.\n",
        "\n",
        "Pr√©paration des donn√©es  \n",
        "Les images et les fichiers XML sont appari√©s √† l‚Äôaide du nom de fichier (stem), apr√®s normalisation afin d‚Äô√©viter les probl√®mes li√©s aux diff√©rences de casse ou de s√©parateurs.  \n",
        "Seules les images disposant d‚Äôune annotation valide sont utilis√©es pour l‚Äôentra√Ænement du d√©tecteur, ce qui garantit un apprentissage enti√®rement supervis√©.\n",
        "\n",
        "Un d√©coupage train / validation / test est effectu√© au niveau image afin d‚Äô√©viter toute fuite de donn√©es. La r√©partition est la suivante :  \n",
        "‚Äì 80 % des images pour l‚Äôentra√Ænement (1064 images)  \n",
        "‚Äì 10 % pour la validation (133 images)  \n",
        "‚Äì 10 % pour le test (134 images)  \n",
        "\n",
        "Les cl√©s correspondant √† chaque split sont enregistr√©es dans des fichiers texte s√©par√©s, ce qui permet de reproduire exactement le m√™me d√©coupage ult√©rieurement.\n",
        "\n",
        "Dataset PyTorch pour la d√©tection  \n",
        "Un dataset personnalis√© est impl√©ment√© afin de charger dynamiquement les images RAW et leurs annotations XML.  \n",
        "Pour chaque image, les bounding boxes et les labels sont extraits √† partir du fichier XML et convertis dans un format compatible avec Faster R-CNN.  \n",
        "La classe 0 est r√©serv√©e au fond (background), et les classes r√©elles sont index√©es √† partir de 1, conform√©ment aux conventions de torchvision. Les bounding boxes invalides sont filtr√©es automatiquement.\n",
        "\n",
        "Mod√®le de d√©tection  \n",
        "Le mod√®le utilis√© est Faster R-CNN avec un backbone ResNet-50 et un Feature Pyramid Network (FPN). Les poids du backbone sont pr√©-entra√Æn√©s sur le jeu de donn√©es COCO, ce qui permet d‚Äôacc√©l√©rer la convergence et d‚Äôam√©liorer les performances.  \n",
        "La t√™te de classification du mod√®le est remplac√©e afin de correspondre exactement aux 20 classes du jeu de donn√©es, auxquelles s‚Äôajoute la classe background.\n",
        "\n",
        "Entra√Ænement  \n",
        "L‚Äôentra√Ænement est r√©alis√© √† l‚Äôaide de l‚Äôoptimiseur SGD, avec un learning rate initial de 0.005, un momentum de 0.9 et un weight decay de 5e-4.  \n",
        "Un scheduler de type StepLR est utilis√© afin de r√©duire progressivement le learning rate tous les trois epochs.  \n",
        "Le batch size est fix√© √† 2, ce qui est adapt√© √† l‚Äôarchitecture Faster R-CNN et aux contraintes de m√©moire GPU.\n",
        "\n",
        "√Ä chaque epoch, le mod√®le est entra√Æn√© sur le jeu d‚Äôapprentissage puis √©valu√© sur le jeu de validation. Le mod√®le pr√©sentant la meilleure loss de validation est automatiquement sauvegard√©.\n",
        "\n",
        "Sauvegarde et sortie du mod√®le  \n",
        "Le meilleur mod√®le de d√©tection est sauvegard√© dans le fichier Final_product/models/best_faster_rcnn_raw.pth.  \n",
        "Ce fichier contient les poids du mod√®le ainsi que les informations n√©cessaires √† l‚Äôinf√©rence, notamment la liste des classes et le dictionnaire de correspondance classe‚Äìindice.\n",
        "\n",
        "R√¥le dans le pipeline global  \n",
        "Le d√©tecteur constitue la premi√®re √©tape du pipeline. Il permet de localiser les a√©ronefs dans une image et de g√©n√©rer des bounding boxes pr√©cises.  \n",
        "Ces bounding boxes sont ensuite utilis√©es pour extraire des crops, qui sont transmis √† un second mod√®le de classification bas√© sur ResNet-50. Cette s√©paration entre d√©tection et classification fine permet d‚Äôam√©liorer la robustesse et la pr√©cision globale du syst√®me, en particulier pour des classes visuellement proches.\n"
      ],
      "metadata": {
        "id": "jCpy1jsw2Yoj"
      }
    }
  ]
}